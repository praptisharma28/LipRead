{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential # To initialise the nn as a sequence of layers\n",
    "from keras.layers import Convolution2D # To make the convolution layer for 2D images\n",
    "from keras.layers import MaxPooling2D # \n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "csv=CSVLogger(\"8_preprocessing_model.log\")\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32,(2,2),input_shape = (224,224,1),strides=2,name='convo1'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Convolution2D(64,(3,3), activation = 'relu',name='convo2'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(64,(3,3),activation = 'relu',name='convo3'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# classifier.add(BatchNormalization())\n",
    "# As GlobalAveragePooling2D take the average of all the dimensions((26, 26, 64) --> (64)), so normalizing at this \n",
    "# spot seems misleading.\n",
    "classifier.add(GlobalAveragePooling2D())\n",
    "# As our model is still facing the problem so, we need to increase the regulization\n",
    "classifier.add(Dropout((0.5)))\n",
    "classifier.add(Dense(20, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convo1 (Conv2D)              (None, 112, 112, 32)      160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "convo2 (Conv2D)              (None, 110, 110, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "convo3 (Conv2D)              (None, 53, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 57,012\n",
      "Trainable params: 56,948\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2600 images belonging to 20 classes.\n",
      "Found 200 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(epsilon = 0.01,lr = 0.001)\n",
    "classifier.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,featurewise_center = True,samplewise_center = True,\n",
    "                                   featurewise_std_normalization = True,samplewise_std_normalization = True,\n",
    "                                  width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "basefolder = os.path.dirname(curr_path)\n",
    "\n",
    "train_folder = os.path.join(basefolder, \"Dataset\\\\Train\")\n",
    "test_folder = os.path.join(basefolder, \"Dataset\\\\dev\")\n",
    "\"\"\"\n",
    "# Changes for linux\n",
    "train_folder = os.path.join(basefolder, \"Dataset/Train\")\n",
    "test_folder = os.path.join(basefolder, \"Dataset/dev\")\n",
    "\"\"\"\n",
    "train_set = train_datagen.flow_from_directory(train_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:506: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "C:\\Users\\dhruv\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\preprocessing\\image.py:514: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn'tbeen fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400s - loss: 2.9960 - acc: 0.0452 - val_loss: 3.3831 - val_acc: 0.0503\n",
      "Epoch 2/10\n",
      "395s - loss: 2.9960 - acc: 0.0444 - val_loss: 3.3832 - val_acc: 0.0498\n",
      "Epoch 3/10\n",
      "395s - loss: 2.9960 - acc: 0.0457 - val_loss: 3.3825 - val_acc: 0.0501\n",
      "Epoch 4/10\n",
      "387s - loss: 2.9960 - acc: 0.0442 - val_loss: 3.3846 - val_acc: 0.0496\n",
      "Epoch 5/10\n",
      "378s - loss: 2.9960 - acc: 0.0452 - val_loss: 3.3856 - val_acc: 0.0494\n",
      "Epoch 6/10\n",
      "381s - loss: 2.9960 - acc: 0.0446 - val_loss: 3.3820 - val_acc: 0.0501\n",
      "Epoch 7/10\n",
      "392s - loss: 2.9960 - acc: 0.0463 - val_loss: 3.3858 - val_acc: 0.0496\n",
      "Epoch 8/10\n",
      "403s - loss: 2.9960 - acc: 0.0458 - val_loss: 3.3855 - val_acc: 0.0500\n",
      "Epoch 9/10\n",
      "405s - loss: 2.9960 - acc: 0.0455 - val_loss: 3.3839 - val_acc: 0.0501\n",
      "Epoch 10/10\n",
      "394s - loss: 2.9960 - acc: 0.0457 - val_loss: 3.3845 - val_acc: 0.0501\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(train_set,steps_per_epoch=2600,epochs=10,validation_data=test_set,validation_steps=200,callbacks=[csv],verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

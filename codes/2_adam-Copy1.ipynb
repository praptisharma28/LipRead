{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential # To initialise the nn as a sequence of layers\n",
    "from keras.layers import Convolution2D # To make the convolution layer for 2D images\n",
    "from keras.layers import MaxPooling2D # \n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "csv=CSVLogger(\"2_adam-Copy1.log\")\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32,(3,3),input_shape = (224,224,1), activation = 'sigmoid',name='convo2'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32,(3,3),activation = 'sigmoid',name='convo3'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Convolution2D(64,(3,3),activation = 'relu',name='convo4'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "# Step 3 - Flatten\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dropout((0.6)))\n",
    "classifier.add(Dense(1024))\n",
    "classifier.add(BatchNormalization(scale = False))\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout((0.4)))\n",
    "classifier.add(Dense(20, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "convo2 (Conv2D)              (None, 222, 222, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "convo3 (Conv2D)              (None, 109, 109, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "convo4 (Conv2D)              (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              44303360  \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1024)              3072      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 44,354,996\n",
      "Trainable params: 44,352,948\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2600 images belonging to 20 classes.\n",
      "Found 200 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier.compile(optimizer = \"Adam\", loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "curr_path = os.getcwd()\n",
    "basefolder = os.path.dirname(curr_path)\n",
    "train_folder = os.path.join(basefolder, \"Dataset\\\\Train\")\n",
    "test_folder = os.path.join(basefolder, \"Dataset\\\\dev\")\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(train_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_folder,target_size=(224, 224),batch_size=32,class_mode='categorical',color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2600/2600 [==============================] - 552s 212ms/step - loss: 3.2932 - acc: 0.0483 - val_loss: 2.9958 - val_acc: 0.0496\n",
      "Epoch 2/10\n",
      "2600/2600 [==============================] - 543s 209ms/step - loss: 3.1173 - acc: 0.0476 - val_loss: 2.9958 - val_acc: 0.0499\n",
      "Epoch 3/10\n",
      "2600/2600 [==============================] - 538s 207ms/step - loss: 0.8278 - acc: 0.7587 - val_loss: 2.8665 - val_acc: 0.3809\n",
      "Epoch 4/10\n",
      "2600/2600 [==============================] - 538s 207ms/step - loss: 0.0264 - acc: 0.9944 - val_loss: 3.0969 - val_acc: 0.4408\n",
      "Epoch 5/10\n",
      "2600/2600 [==============================] - 540s 208ms/step - loss: 0.0189 - acc: 0.9953 - val_loss: 3.2784 - val_acc: 0.4401\n",
      "Epoch 6/10\n",
      "2600/2600 [==============================] - 539s 207ms/step - loss: 0.0154 - acc: 0.9956 - val_loss: 3.5474 - val_acc: 0.4206\n",
      "Epoch 7/10\n",
      "2600/2600 [==============================] - 540s 208ms/step - loss: 0.0134 - acc: 0.9961 - val_loss: 3.9565 - val_acc: 0.4392\n",
      "Epoch 8/10\n",
      "2600/2600 [==============================] - 539s 207ms/step - loss: 0.0143 - acc: 0.9957 - val_loss: 4.5268 - val_acc: 0.3947\n",
      "Epoch 9/10\n",
      "2600/2600 [==============================] - 541s 208ms/step - loss: 0.0103 - acc: 0.9971 - val_loss: 3.9119 - val_acc: 0.4462\n",
      "Epoch 10/10\n",
      "2600/2600 [==============================] - 541s 208ms/step - loss: 0.0106 - acc: 0.9967 - val_loss: 4.5883 - val_acc: 0.4553\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit_generator(train_set,steps_per_epoch=2600,epochs=10,validation_data=test_set,validation_steps=200,callbacks=[csv],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"2_adam-Copy1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
